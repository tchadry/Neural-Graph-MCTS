# Neural-MCTS

Recently, many research proposed novel approaches to develop systems that can learn how to play games based solely on self-play and does not require any human expert domain knowledge. So, through a number of episode plays and simulations, one can generate training examples to the model or evaluate a heuristic for non-terminal states. Some of these models even achieved superhuman performance like the AlphaGo Zero. More specifically we focus on the Travelling Salesman Problem, but a similar approach may be utilized for other NP-Hard problems as well.
Specifically, we are going to focus on the Travelling Salesman Problem to demonstrate our method. We aim to solve the problem in three ways - first by finding the optimal exponential solution for problems with up to ten nodes (or for higher number of nodes, approximate using an existing TSP solver). We will also implement an MCTS algorithm that is able to solve the problem and get a close-to-optimal solutions for higher number of nodes. Finally, we create a Graph Neural Network to be the estimated policy that guides the MCTS search algorithm on the TSP problem, which will perform policy iteration over the GNN through self play.
Given the game formulation of the problem, this method does not need domain knowledge to solve NP-Hard problems, which is favorable since the algorithm becomes extensible to other kinds of problems as previously mentioned.
For small number of nodes, we are going to solve by testing all possible paths and compare the most optimal to the GNN + MCTS solutions. As an example, the MCTS algorithm alone is able to solve TSP instances with large number of nodes, if able to perform enough iterations. However, we expect to, and will evaluate the performance of the GNN, based on getting better results and still being able to reduce the number of MCTS iterations.
